PS C:\Users\btina\LLM\AI-Langchain\src> python serve.py
2024-03-15 14:21:22 - INFO - fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.
2024-03-15 14:21:22 - DEBUG - Starting new HTTPS connection (1): docs.smith.langchain.com:443
2024-03-15 14:21:23 - DEBUG - https://docs.smith.langchain.com:443 "GET /user_guide HTTP/1.1" 200 57648
2024-03-15 14:21:23 - DEBUG - Encoding detection: utf_8 is most likely the one.
2024-03-15 14:21:23 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-03-15 14:21:23 - DEBUG - load_verify_locations cafile='C:\\Python312\\Lib\\site-packages\\certifi\\cacert.pem'
2024-03-15 14:21:23 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-03-15 14:21:23 - DEBUG - load_verify_locations cafile='C:\\Python312\\Lib\\site-packages\\certifi\\cacert.pem'
2024-03-15 14:21:24 - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000026226756F20>, 'json_data': {'input': [[27317, 42985, 2724, 13002, 765, 11410, 99, 250, 31643, 9468, 249, 254, 31643, 23272, 42985], [36234, 311, 1925, 2262, 9468, 99, 250, 31643, 9468, 249, 254, 31643, 23272, 42985, 61791, 27317, 19368, 13325, 61791, 27317, 19368, 12438, 14, 10155, 61791, 27317, 42985, 5446, 61791, 6014, 11087, 311, 1883, 27317, 42985, 1502, 13002, 22574, 47, 39906, 320, 57789, 32862, 8, 12363, 12, 86021, 1305, 4628, 83463, 99162, 55715, 27636, 16617, 1502, 13002, 1966, 420, 2199, 27317, 42985, 2724, 13002, 27317, 42985, 374, 264, 5452, 369, 445, 11237, 3851, 4500, 11, 16967, 11, 323, 7649, 13, 763, 420, 8641, 11, 584, 4805, 11415, 279, 58321, 315, 88568, 23272, 42985, 11815, 323, 1268, 814, 5052, 1139, 1855, 6566, 315, 279, 3851, 4500, 48608, 13, 1226, 3987, 420, 690, 6179, 3932, 1268, 311, 1888, 29166, 420, 8147, 5452, 477, 3041, 1124, 2555, 311, 2980, 422, 814, 3207, 1120, 6041, 872, 11879, 43925, 67247, 16067, 12705, 67247, 445, 11237, 8522, 3629, 18065, 4062, 66196, 1990, 52032, 11, 1646, 4595, 11, 57470, 8446, 323, 1023, 5137, 627, 791, 5845, 311, 19019, 3619, 1268, 279, 1646, 374, 16785, 2001, 323, 7542, 1405, 433, 374, 22109, 2001, 374, 17235, 3062, 369, 420, 10474, 20967, 3252, 16067, 4599, 11469, 502, 445, 11237, 8522, 11, 584, 4284, 3515, 23272, 42985, 46515, 9147, 555, 1670, 627, 46, 728, 92455, 11, 433, 4536, 1431, 5995, 311, 1427, 520, 1475, 3254, 11917, 13, 4452, 11, 994, 2574, 733, 5076, 320, 276, 16907, 842, 1121, 11, 24746, 8479, 6471, 11, 29493, 1109, 3685, 11572, 11, 5190, 1109, 3685, 4037, 10648, 705, 433, 753, 9193, 11190, 311, 7542, 555, 3411, 1555, 279, 3851, 35483, 13, 23272, 42985, 6835, 2867, 24035, 323, 28803, 2038, 520, 1855, 3094, 315, 459, 445, 11237, 8668, 11, 3339, 433, 1790, 8831, 311, 10765, 323, 3789, 12, 1593, 4819, 627, 1687, 3493, 10068, 21568, 315, 6369, 6743, 11, 5865, 11, 323, 17622, 9477, 34032, 3475, 2638, 16067, 8142, 1690, 13707, 2103, 8448, 459, 2926, 2373, 315, 872, 3851, 3196, 389, 1054, 85, 24459, 12621, 9520, 584, 4070, 3970, 459, 7859, 1396, 315, 15009, 7411, 1212, 311, 11142, 264, 810, 1296, 16625, 5603, 13, 23272, 42985, 6276, 13707, 311, 1893, 30525, 11, 902, 527, 15661, 315, 11374, 323, 5905, 16674, 11, 323, 1005, 1521, 311, 1629, 7177, 389, 872, 445, 11237, 8522, 627, 9673, 1296, 5157, 649, 387, 23700, 304, 20155, 11, 3549, 389, 279, 11722, 11, 477, 35990, 505, 3851, 35483, 13, 23272, 42985, 1101, 3727, 433, 4228, 311, 1629, 2587, 56181, 320, 21704, 445, 11237, 323, 67709, 3196, 8, 311, 5573, 1296, 3135, 3034, 36642, 2806, 16067, 4599, 1760, 67247, 2204, 11028, 315, 701, 8522, 323, 3339, 4442, 11, 433, 753, 3062, 311, 1518, 3508, 477, 539, 499, 4070, 1239, 14715, 449, 5201, 311, 701, 2926, 1296, 5157, 627, 46, 728, 92455, 11, 4442, 304, 279, 10137, 11, 57470, 8446, 11, 477, 1646, 5873, 649, 617, 6908, 25127, 304, 14847, 9124, 555, 701, 3851, 627, 644, 2015, 311, 636, 264, 5647, 369, 902, 11678, 374, 16785, 2731, 11, 433, 753, 5505, 311, 387, 3025, 311, 1684, 3135, 369, 2204, 33483, 389, 279, 1890, 62823, 14737, 3185, 14656, 25034, 13, 1226, 4070, 29091, 17345, 304, 264, 1217, 22658, 12593, 1684, 369, 1296, 8640, 311, 3839, 323, 58681, 70880, 919, 304, 1296, 12483, 4028, 5361, 54862, 315, 701, 3851, 25469, 1993, 16067, 27317, 42985, 5825, 264, 42715, 4676, 369, 11295, 20140, 323, 66196, 627, 2028, 6276, 499, 311, 6288, 1296, 704, 2204, 52032, 323, 4211, 13, 1472, 649, 1825, 279, 42715, 505, 904, 10137, 477, 1646, 1629, 304, 701, 11917, 13], [2028, 6276, 499, 311, 6288, 1296, 704, 2204, 52032, 323, 4211, 13, 1472, 649, 1825, 279, 42715, 505, 904, 10137, 477, 1646, 1629, 304, 701, 11917, 627, 11769, 42715, 1629, 374, 14042, 304, 279, 1887, 323, 649, 387, 1511, 311, 1893, 1296, 5157, 477, 9616, 449, 1023, 8640, 1823, 1955, 27866, 16067, 65911, 7649, 6276, 13707, 311, 6667, 810, 828, 389, 1268, 872, 445, 11237, 8522, 527, 16785, 304, 1972, 31184, 26350, 13, 763, 420, 10474, 11, 433, 753, 3062, 311, 2274, 459, 8830, 369, 279, 4595, 315, 11374, 279, 917, 374, 16785, 1664, 477, 31555, 389, 323, 1268, 7041, 433, 753, 15061, 1523, 304, 1884, 5157, 13, 11995, 11302, 4526, 323, 1629, 21917, 527, 9200, 369, 420, 29388, 13, 1115, 690, 1520, 304, 272, 2060, 315, 1296, 5157, 430, 649, 1520, 3839, 70880, 919, 73124, 782, 12760, 323, 4500, 315, 17392, 56181, 732, 25627, 287, 37957, 16067, 4599, 28818, 701, 3851, 311, 459, 2926, 743, 315, 3932, 11, 433, 753, 3062, 311, 9762, 3823, 11302, 389, 279, 14847, 433, 753, 17843, 13, 1115, 8779, 4128, 6666, 311, 279, 1455, 7185, 8640, 323, 11415, 6964, 5157, 430, 527, 14718, 36033, 14847, 13, 23272, 42985, 6276, 499, 311, 15866, 11302, 12483, 311, 14042, 35483, 320, 14450, 92455, 11, 420, 374, 43770, 709, 311, 264, 11302, 3215, 304, 701, 917, 705, 1243, 4141, 389, 35483, 430, 617, 264, 3230, 11302, 4877, 323, 5573, 13, 362, 4279, 29388, 374, 311, 4141, 389, 35483, 430, 5371, 264, 8009, 1217, 11302, 5573, 11, 1243, 31646, 1523, 1139, 36033, 3585, 1701, 279, 11944, 11917, 1684, 8794, 1962, 1113, 1183, 2492, 16067, 27317, 42985, 1101, 11815, 11889, 8640, 311, 21917, 49194, 11, 902, 2187, 37142, 3046, 311, 15499, 25052, 7185, 35483, 323, 89498, 1124, 449, 5201, 311, 2204, 13186, 13, 1556, 1962, 3046, 649, 387, 5975, 82, 11, 25175, 11, 477, 1524, 3917, 5030, 11909, 13, 1115, 6276, 3932, 311, 2339, 70880, 919, 4028, 3062, 16865, 13186, 1943, 287, 51090, 311, 264, 40283, 16067, 2170, 701, 3851, 68711, 1555, 279, 13746, 7649, 10474, 11, 433, 596, 7718, 311, 3136, 26984, 828, 311, 46464, 323, 7417, 1202, 5178, 13, 23272, 42985, 20682, 499, 311, 923, 8640, 439, 10507, 311, 30525, 320, 1527, 2225, 279, 2447, 2199, 323, 2949, 459, 21917, 7325, 705, 24050, 701, 1296, 10401, 389, 1972, 31184, 26350, 13, 1115, 374, 264, 1401, 8935, 304, 3515, 701, 8558, 1887, 323, 701, 16865, 29901, 1887, 304, 279, 1890, 5452, 1087, 17158, 16067, 34, 2353, 989, 25052, 287, 1401, 828, 3585, 11, 7982, 29531, 287, 30525, 11, 37142, 1113, 35483, 11, 323, 39662, 1523, 1139, 3062, 828, 304, 11917, 1684, 527, 88568, 499, 4805, 1101, 1390, 311, 656, 3131, 701, 917, 13280, 5788, 13, 4452, 11, 5423, 520, 279, 5788, 6566, 11, 433, 753, 16996, 311, 636, 264, 1579, 11852, 24131, 315, 3851, 5178, 449, 5201, 311, 40370, 11, 2853, 11, 323, 11302, 12483, 13, 1115, 26420, 430, 433, 596, 24944, 35946, 3135, 520, 5569, 1345, 31414, 287, 323, 362, 16675, 27866, 16067, 27317, 42985, 5825, 16967, 27223, 430, 2187, 499, 311, 3839, 1401, 17150, 927, 892, 13, 1472, 649, 9407, 311, 1684, 17150, 369, 264, 2728, 4261, 323, 31646, 1523, 1139, 264, 3230, 828, 1486, 311, 636, 264, 11917, 2007, 369, 430, 892, 4261, 2001, 420, 374, 5423, 26222, 369, 28803, 5788, 4819, 79016, 42985, 1101, 6276, 369, 4877, 323, 11408, 50284, 11, 902, 6276, 3932, 311, 1906, 2204, 11028, 315, 872, 8522, 449, 2204, 48316, 323, 1684, 1268, 814, 527, 16785, 3185, 14656, 25034, 2949, 1855, 9676, 13, 1115, 374, 11190, 369, 362, 16675, 7649, 4442, 304, 10137, 11, 1646, 11, 477, 57470, 8446, 1196, 300, 420, 2199, 11190, 30, 21994, 27317, 42985, 5971, 22574, 12705, 67247, 65911, 27866, 46067, 34868, 24374, 541, 25904, 76715, 64207, 6247, 27317, 42985, 27721, 31380, 12830, 14, 10155, 7816, 97552, 27798, 13857, 7388, 220, 2366, 19, 23272, 19368, 11, 4953, 13]], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2024-03-15 14:21:24 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2024-03-15 14:21:24 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002622740E1E0>
2024-03-15 14:21:24 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000262265E17D0> server_hostname='api.openai.com' timeout=None
2024-03-15 14:21:24 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026226592870>
2024-03-15 14:21:24 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-03-15 14:21:24 - DEBUG - send_request_headers.complete
2024-03-15 14:21:24 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-03-15 14:21:24 - DEBUG - send_request_body.complete
2024-03-15 14:21:24 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-03-15 14:21:24 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 15 Mar 2024 21:21:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-rlp2xd2b0hrrhzev6mkoq5sg'), (b'openai-processing-ms', b'41'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'998769'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_5d8565403f68c36c4b87d684f5437614'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=kGERtBvWtx04O1Jz_W0ga0B3afklMqurXTGvcdw7vQs-1710537685-1.0.1.1-XhDdNArkzD__GhoFd_43NlbeaM7EDIIUizqFFDq71I9cgRutAH2EJJxkERMa7WmdcXGBSWBw.ypTmt8ypumSDQ; path=/; expires=Fri, 15-Mar-24 21:51:25 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=6I576EsGu0QFSYHqEdjZ6HxFYJVGWxDjlb9VhgshpDo-1710537685453-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'864f8d954ca108ad-SEA'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-03-15 14:21:24 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2024-03-15 14:21:24 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-03-15 14:21:24 - DEBUG - receive_response_body.complete
2024-03-15 14:21:24 - DEBUG - response_closed.started
2024-03-15 14:21:24 - DEBUG - response_closed.complete
2024-03-15 14:21:24 - DEBUG - HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2024-03-15 14:21:24 - DEBUG - Environment variable FAISS_OPT_LEVEL is not set, so let's pick the instruction set according to the current CPU
2024-03-15 14:21:24 - INFO - Loading faiss with AVX512 support.
2024-03-15 14:21:24 - INFO - Could not load library with AVX512 support due to:
ModuleNotFoundError("No module named 'faiss.swigfaiss_avx512'")
2024-03-15 14:21:24 - INFO - Loading faiss with AVX2 support.
2024-03-15 14:21:24 - INFO - Successfully loaded faiss with AVX2 support.
2024-03-15 14:21:24 - DEBUG - Past, retriever
2024-03-15 14:21:24 - DEBUG - Past, create tools
2024-03-15 14:21:24 - DEBUG - XX
2024-03-15 14:21:24 - DEBUG - Starting new HTTPS connection (1): api.hub.langchain.com:443
2024-03-15 14:21:26 - DEBUG - https://api.hub.langchain.com:443 "GET /commits/hwchase17/openai-functions-agent/?limit=100&offset=0 HTTP/1.1" 200 None
2024-03-15 14:21:26 - DEBUG - Starting new HTTPS connection (1): api.hub.langchain.com:443
2024-03-15 14:21:27 - DEBUG - https://api.hub.langchain.com:443 "GET /commits/hwchase17/openai-functions-agent/a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5 HTTP/1.1" 200 None
2024-03-15 14:21:27 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-03-15 14:21:27 - DEBUG - load_verify_locations cafile='C:\\Python312\\Lib\\site-packages\\certifi\\cacert.pem'
2024-03-15 14:21:27 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-03-15 14:21:27 - DEBUG - load_verify_locations cafile='C:\\Python312\\Lib\\site-packages\\certifi\\cacert.pem'
2024-03-15 14:21:27 - DEBUG - Past, create agent
2024-03-15 14:21:27 - DEBUG - Past, app definition
2024-03-15 14:21:27 - DEBUG - Using proactor: IocpProactor
INFO:     Started server process [8980]
INFO:     Waiting for application startup.

 __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______
|  |        /   \     |  \ |  |  /  _____|    /       ||   ____||   _  \    \   \  /   / |   ____|
|  |       /  ^  \    |   \|  | |  |  __     |   (----`|  |__   |  |_)  |    \   \/   /  |  |__
|  |      /  /_\  \   |  . `  | |  | |_ |     \   \    |   __|  |      /      \      /   |   __|
|  `----./  _____  \  |  |\   | |  |__| | .----)   |   |  |____ |  |\  \----.  \    /    |  |____
|_______/__/     \__\ |__| \__|  \______| |_______/    |_______|| _| `._____|   \__/     |_______|

LANGSERVE: Playground for chain "/agent/" is live at:
LANGSERVE:  ‚îÇ
LANGSERVE:  ‚îî‚îÄ‚îÄ> /agent/playground/
LANGSERVE:
LANGSERVE: See all available routes at /docs/

LANGSERVE: ‚ö†Ô∏è Using pydantic 2.6.3. OpenAPI docs for invoke, batch, stream, stream_log endpoints will not be generated. API endpoints and playground should work as expected. If you need to see the docs, you can downgrade to pydantic 1. For example, `pip install pydantic==1.10.13`. See https://github.com/tiangolo/fastapi/issues/10360 for details.

INFO:     Application startup complete.
INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)
INFO:     ::1:5442 - "GET /agent/playground HTTP/1.1" 307 Temporary Redirect
INFO:     ::1:5442 - "GET /agent/playground/ HTTP/1.1" 200 OK
INFO:     ::1:5442 - "GET /agent/playground/assets/index-dbc96538.js HTTP/1.1" 200 OK
INFO:     ::1:5443 - "GET /agent/playground/assets/index-52e8ab2f.css HTTP/1.1" 200 OK
INFO:     ::1:5443 - "GET /agent/playground/favicon.ico HTTP/1.1" 200 OK


> Entering new AgentExecutor chain...
2024-03-15 14:22:45 - DEBUG - Using proactor: IocpProactor
2024-03-15 14:22:45 - DEBUG - Using proactor: IocpProactor
2024-03-15 14:22:45 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'content': 'how can langsmith help with testing?'}], 'model': 'gpt-4-1106-preview', 'functions': [{'name': 'langsmith_search', 'description': 'Search for information about LangSmith. For any questions about LangSmith, you must use this tool!', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}, {'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}], 'n': 1, 'stream': True, 'temperature': 0.0}}
2024-03-15 14:22:45 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2024-03-15 14:22:45 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000262274CB320>
2024-03-15 14:22:45 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026226809C50> server_hostname='api.openai.com' timeout=None
2024-03-15 14:22:45 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002622749F2F0>
2024-03-15 14:22:45 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-03-15 14:22:45 - DEBUG - send_request_headers.complete
2024-03-15 14:22:45 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-03-15 14:22:45 - DEBUG - send_request_body.complete
2024-03-15 14:22:45 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-03-15 14:22:46 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 15 Mar 2024 21:22:47 GMT'), (b'Content-Type', b'text/event-stream'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-4-1106-preview'), (b'openai-organization', b'user-rlp2xd2b0hrrhzev6mkoq5sg'), (b'openai-processing-ms', b'616'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'300000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'299965'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'6ms'), (b'x-request-id', b'req_09833bc65a6c70000d3956a56ca8fc0c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=L2de1rh1P0D0YibchjHhEyponnK8193ll7bmhdP5gMQ-1710537767-1.0.1.1-ElJxOWXD3cg5ZC6T3qezl6iYSdIxrY6sbZ4WBW6JjAEXrjRUnoHHBGiZhcq9c9ydn9FskMnpMGuA2r9W9tnOdw; path=/; expires=Fri, 15-Mar-24 21:52:47 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=VsByKH_4wF9vM8pOIxriU4X51zvEBpeqb0nDH_YvF9o-1710537767263-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'864f8f90bc5cc3cd-SEA'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-03-15 14:22:46 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 14:22:46 - DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-03-15 14:22:46 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-03-15 14:22:46 - DEBUG - receive_response_body.complete
2024-03-15 14:22:46 - DEBUG - response_closed.started
2024-03-15 14:22:46 - DEBUG - response_closed.complete

Invoking: `langsmith_search` with `{'query': 'how can langsmith help with testing'}`


2024-03-15 14:22:46 - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function AsyncEmbeddings.create.<locals>.parser at 0x000002622849FBA0>, 'json_data': {'input': [[5269, 649, 8859, 34117, 1520, 449, 7649]], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2024-03-15 14:22:46 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2024-03-15 14:22:46 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026228473CB0>
2024-03-15 14:22:46 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000262158EBA50> server_hostname='api.openai.com' timeout=None
2024-03-15 14:22:46 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026228472840>
2024-03-15 14:22:46 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-03-15 14:22:46 - DEBUG - send_request_headers.complete
2024-03-15 14:22:46 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-03-15 14:22:46 - DEBUG - send_request_body.complete
2024-03-15 14:22:46 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-03-15 14:22:47 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 15 Mar 2024 21:22:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-rlp2xd2b0hrrhzev6mkoq5sg'), (b'openai-processing-ms', b'26'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999993'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_fd151f038210dff14de8851ae566b9e4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=U_59Iw6uZaBR_SYvcVOoyKlbsDTy.X5yLfD1LLw5LcM-1710537767-1.0.1.1-EB7Juqf9RIXZHfqXo5dU2Lk4NAeEXHIC53GAeORX2M75bUWV8hY4FIfudnHZ.75G4wX08uTDZOVrPLBEMG76Sg; path=/; expires=Fri, 15-Mar-24 21:52:47 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=mGKUpZ7HbbUIdnGaEy_U3iJe8.6b5aKo_zA31kPWrcY-1710537767899-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'864f8f979e809354-SEA'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-03-15 14:22:47 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2024-03-15 14:22:47 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-03-15 14:22:47 - DEBUG - receive_response_body.complete
2024-03-15 14:22:47 - DEBUG - response_closed.started
2024-03-15 14:22:47 - DEBUG - response_closed.complete
2024-03-15 14:22:47 - DEBUG - HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
Skip to main contentü¶úÔ∏èüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Com
ing Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.Prototyping‚ÄãPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval sstrategy and other parameters.
The ability to rapidly understand how the model is performing ‚Äî and debug where it is failing ‚Äî is incredibly important for this phase.Debugging‚ÄãWhen ddeveloping new LLM applications, we suggest having LangSmith tracing enabled by default.
Oftentimes, it isn‚Äôt necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it‚Äôs extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.
We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set‚ÄãWhile many developers still ship an initial version off their application based on ‚Äúvibe checks‚Äù, we‚Äôve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.      
These test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View‚ÄãWhen prototyping different versions of your applications and making changes, itt‚Äôs important to see whether or not you‚Äôve regressed with respect to your initial test cases.
Oftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.
In order to get a sense for which variant is performing better, it‚Äôs useful to be able to view results for different configurations on the same datapoints side-by-side. We‚Äôve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground‚ÄãLangSmith provides a playground environment for rapid iteration and experimentation.
This allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.

This allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.
Every playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing‚ÄãBeta testing allows developerrs to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it‚Äôs important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it‚Äôs breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Collecting Feedback‚ÄãWhen launching your application to an initial set of users, it‚Äôs important to gather human feedback on the responsees it‚Äôs producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces‚ÄãLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspecct interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset‚ÄãAs your application progresses through the beta testing phasse, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production‚ÄãClosely inspecting key data points, growing benchmarking datasets, annotatinng traces, and drilling down into important data in trace view are workflows you‚Äôll also want to do once your app hits production. However, especially at the production stage, it‚Äôs crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Monitoring and A/B Testing‚ÄãLangSmith provides monitoring charts that allow you to track key metrrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period ‚Äî this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Was this page helpful?PreviousLangSmithNextSetupPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.

LangSmith User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith 2024-03-15 14:22:47 - DEBUG - Using proactor: IocpProactor
2024-03-15 14:22:47 - DEBUG - Using proactor: IocpProactor
2024-03-15 14:22:47 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'content': 'how can langsmith help with testing?'}, {'role': 'assistant', 'content': None, 'function_call': {'arguments': '{"query":"how can langsmith help with testing"}', 'name': 'langsmith_search'}}, {'role': 'function', 'content': "Skip to main contentü¶úÔ∏èüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Comin 
g Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.Prototyping\u200bPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\nThe ability to rapidly understand how the model is performing ‚Äî and debug where it is failing ‚Äî is incredibly important for this phase.Debugging\u200bWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\nOftentimes, it isn‚Äôt necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it‚Äôs extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set\u200bWhile many developers still ship an initial version of their application based on ‚Äúvibe checks‚Äù, we‚Äôve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View\u200bWhen prototyping different versions of your applications and making changes, it‚Äôs important to see whether or not you‚Äôve regressed with respect to your initial test cases.\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\nIn order to get a sense for which variant is performing better, it‚Äôs useful to be able to view results for different configurations on the same datapoints side-by-side. We‚Äôve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground\u200bLangSmith provides a playground environment for rapid iteration and experimentation.\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.\n\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.\nEvery playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing\u200bBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it‚Äôs important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it‚Äôs breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Collecting Feedback\u200bWhen launching your application to an initial set of users, it‚Äôs important to gather human feedback on the responses it‚Äôs producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces\u200bLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset\u200bAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production\u200bClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you‚Äôll also want to do once your app hits production. However, especially at the production stage, it‚Äôs crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Monitoring and A/B Testing\u200bLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period ‚Äî this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Was this page helpful?PreviousLangSmithNextSetupPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.\n\nLangSmith User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith", 'name': 'langsmith_search'}], 'model': 'gpt-4-1106-preview', 'functions': [{'name': 'langsmith_search', 'description': 'Search f 
or information about LangSmith. For any questions about LangSmith, you must use this tool!', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}, {'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}], 'n': 1, 'stream': True, 'temperature': 0.0}}
2024-03-15 14:22:47 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-03-15 14:22:47 - DEBUG - send_request_headers.complete
2024-03-15 14:22:47 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-03-15 14:22:47 - DEBUG - send_request_body.complete
2024-03-15 14:22:47 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-03-15 14:22:47 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 15 Mar 2024 21:22:48 GMT'), (b'Content-Type', b'text/event-stream'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-4-1106-preview'), (b'openai-organization', b'user-rlp2xd2b0hrrhzev6mkoq5sg'), (b'openai-processing-ms', b'561'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'300000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'298331'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'333ms'), (b'x-request-id', b'req_563631966c0b75db6b32a2c9e932920b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'864f8f9a8d4fc3cd-SEA'), (b'alt-svc', b'h3=":443"; ma=86400')])        
2024-03-15 14:22:47 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 14:22:47 - DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-03-15 14:22:47 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-03-15 14:22:59 - DEBUG - receive_response_body.complete
2024-03-15 14:22:59 - DEBUG - response_closed.started
2024-03-15 14:22:59 - DEBUG - response_closed.complete
LangSmith can assist with testing in several ways throughout the application development lifecycle:

1. **Initial Test Set**: LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, to run tests on their LLM applications. These test cases can be uploaded in bulk, created on the fly, or exported from application traces. Custom evaluations (both LLM and heuristic-based) can be used to score test results.

2. **Comparison View**: When making changes to applications, it's crucial to know if there's been a regression in performance. LangSmith provides a comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of an application.

3. **Playground**: LangSmith offers a playground environment for rapid iteration and experimentation with different prompts and models. Every playground run is logged and can be used to create test cases or compare with other runs.

4. **Beta Testing**: During beta testing, developers can collect data on how their LLM applications perform in real-world scenarios. Feedback collection and run annotation are critical for this workflow, helping to curate test cases that track regressions or improvements and develop automatic evaluations.

5. **Collecting Feedback**: LangSmith allows attaching feedback scores to logged traces, which can be filtered based on specific feedback tags and scores. This helps identify and focus on problematic responses.

6. **Annotating Traces**: Runs can be sent to annotation queues for detailed inspection and annotation with respect to different criteria by PMs, engineers, or subject matter experts.

7. **Adding Runs to a Dataset**: As the application progresses, runs can be added as examples to datasets, expanding test coverage on real-world scenarios.

8. **Monitoring and A/B Testing**: In production, LangSmith provides monitoring charts to track key metrics over time and supports tag and metadata grouping for A/B testing changes in prompt, model, or retrieval strategy.

Overall, LangSmith supports a test-driven approach to LLM application development, offering tools for creating and managing test cases, evaluating application performance, and refining applications based on real-world feedback and data.

> Finished chain.
INFO:     ::1:5473 - "POST /agent/invoke HTTP/1.1" 200 OK
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [8980]
2024-03-15 14:23:48 - DEBUG - close.started
2024-03-15 14:23:48 - DEBUG - close.complete
PS C:\Users\btina\LLM\AI-Langchain\src> 